<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<HTML>
 <HEAD>
   <TITLE> Getting slammed by a client
   </TITLE>
   <LINK REL="Index" HREF="index.html" >
   <LINK REL="made" HREF="mailto:gemini%40lists.orbitalfox.eu?Subject=Re%3A%20Getting%20slammed%20by%20a%20client&In-Reply-To=%3Calpine.DEB.2.20.2007251013110.16057%40pling.qwghlm.org%3E">
   <META NAME="robots" CONTENT="index,nofollow">
   <style type="text/css">
       pre {
           white-space: pre-wrap;       /* css-2.1, curent FF, Opera, Safari */
           }
   </style>
   <META http-equiv="Content-Type" content="text/html; charset=us-ascii">
   <LINK REL="Previous"  HREF="002336.html">
   <LINK REL="Next"  HREF="002341.html">
 </HEAD>
 <BODY BGCOLOR="#ffffff">
   <H1>Getting slammed by a client</H1>
    <B>Martin Keegan</B> 
    <A HREF="mailto:gemini%40lists.orbitalfox.eu?Subject=Re%3A%20Getting%20slammed%20by%20a%20client&In-Reply-To=%3Calpine.DEB.2.20.2007251013110.16057%40pling.qwghlm.org%3E"
       TITLE="Getting slammed by a client">martin at no.ucant.org
       </A><BR>
    <I>Sat Jul 25 10:18:27 BST 2020</I>
    <P><UL>
        <LI>Previous message (by thread): <A HREF="002336.html">Getting slammed by a client
</A></li>
        <LI>Next message (by thread): <A HREF="002341.html">Getting slammed by a client
</A></li>
         <LI> <B>Messages sorted by:</B> 
              <a href="date.html#2337">[ date ]</a>
              <a href="thread.html#2337">[ thread ]</a>
              <a href="subject.html#2337">[ subject ]</a>
              <a href="author.html#2337">[ author ]</a>
         </LI>
       </UL>
    <HR>  
<!--beginarticle-->
<PRE>On Sat, 25 Jul 2020, Hannu Hartikainen wrote:

&gt;<i> Should we develop low-resource honeypots to exhaust crawler resources? 
</I>&gt;<i> Or start maintaining a community blacklist?
</I>
I'd start with hardening Gemini servers ...

The server I've written has a bunch of hard limits (in particular, on the 
duration of requests and the number of concurrent CGI processes). It might 
be advisable to add some more back-pressure for the case where user-agents 
are generating obviously redundant requests to an excessive degree. I'd 
put that before trying honeypots and blacklists.

Mk

-- 
Martin Keegan, +44 7779 296469, @mk270, <A HREF="https://mk.ucant.org/">https://mk.ucant.org/</A>
</PRE>
















<!--endarticle-->
    <HR>
    <P><UL>
        <!--threads-->
	<LI>Previous message (by thread): <A HREF="002336.html">Getting slammed by a client
</A></li>
	<LI>Next message (by thread): <A HREF="002341.html">Getting slammed by a client
</A></li>
         <LI> <B>Messages sorted by:</B> 
              <a href="date.html#2337">[ date ]</a>
              <a href="thread.html#2337">[ thread ]</a>
              <a href="subject.html#2337">[ subject ]</a>
              <a href="author.html#2337">[ author ]</a>
         </LI>
       </UL>

<hr>
<a href="https://lists.orbitalfox.eu/listinfo/gemini">More information about the Gemini
mailing list</a><br>
</body></html>
